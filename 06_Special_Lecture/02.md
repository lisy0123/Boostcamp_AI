# 캐글 그랜드마스터의 경진대회 노하우 대방출

Kaggle

2010년 설립된 세계에서 가장 유명한 인공지능 대회 플랫폼

2017년 3월 구글 인수

194개국 600만명 이상의 회원 보유

캐글을 즐겨하는 사람을 Kaggler라 함

카카오 아레나

비공개 플랫폼 (카카오 계열사 전용) / 2018년 11월에 처음 대회 시작 / 대회 3개

데이콘

타 기업/기관에게 공개된 플랫폼 / 대회 51개/ 캐글 스타일을 빠르게 적용 중

- 취업 목적, 개인 성장

우승위해 필요한 것

- 파이프라인의 빠른/효율적 반복

  - 1달~2달 동안, 평일 하루 평균 4시간 이상 투자 / 주말 하루 평균 8시간 이상 투자
  - 본인만의 기본 코드 구축
  - 다른 사람 코드 참고, 본인 것으로 소화한 코드 만들기

- 점수 개선 아이디어

  - Notebooks 탭 참고
  - Discussion 탭 참고

- 탄탄한 검증 전략

  - 좋은 모델 (일반화; Generalization 성능이 높은 모델)

    Training set에서 얻은 점수가 Test set에서도 비슷하게 나오는 모델

  - 검증 전략 (Validation Strategy)

    Test set에서 얻은 점수와 Training set에서 얻어진 점수 갭을 줄이는 평가 방법

  - Stratified k-fold

  - 아무리 검증 전략을 잘 세워도 오버 피팅의 위험 존재

    => local CV와 public LB가 함께 올라가는 방법 선택

  - 앙상블

    여러 모델의 예측 결과를 섞어서 예측 성능 향상

    => 싱글 모델 보다 항상 더 좋은 성능을 얻을 수 있음

    - Stratified k-fold 앙상블

    - 다양한 모델 앙상블

      정형 데이터: LightGBM, XGBoost, Neural Networks (NNs) 등

      이미지 데이터: resnet, efficientnet, resnext 등

      텍스트 데이터: LSTM, BERT, GPT2, RoBert

      

